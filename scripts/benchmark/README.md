# Elastic Transport Benchmarking Suite

This directory contains comprehensive benchmarking and profiling tools for the `elastic-transport-js` library.

---

## Quick Start

### Run All Benchmarks

```bash
# Real Transport (actual implementation - RECOMMENDED)
npm run benchmark:real

# Transport performance with statistical analysis
npm run benchmark:transport

# GC tracking and memory analysis  
npm run benchmark:gc

# Object merge strategy comparison
npm run benchmark:merge
```

### Run Profiling

```bash
# Generate CPU flamegraph
npm run benchmark:profile

# Or run directly with specific profiler and benchmark
npx tsx scripts/benchmark/profile-benchmark.ts flame scripts/benchmark/benchmark-transport-mitata.ts
```

---

## What Each Benchmark Does

### 1. **Real Transport Benchmark** (`benchmark-transport-real.js`) ⭐ **RECOMMENDED**

**Purpose:** Tests the ACTUAL Transport implementation as used in production

**What it tests:**
- ✅ Real Serializer (JSON.stringify/parse)
- ✅ Real compression (gzip/gunzip)
- ✅ Real connection pool management
- ✅ Real retry logic with maxRetries
- ✅ Real error handling
- ✅ Real header management (user-agent, accept-encoding, etc.)
- ✅ All Transport features enabled/disabled

**Test scenarios:**
1. Baseline (no compression, no retries)
2. With compression enabled
3. With retries (maxRetries=3)
4. Full features (compression + retries)

**Mock vs. Real:**
- Mock: Connection layer (no actual HTTP)
- Real: ALL Transport logic (serialization, compression, pools, retries)

**Run:** `npm run benchmark:real`

**Output:**
- Operations per second for each scenario
- Average latency comparison
- Performance overhead analysis

**Key insight:** Shows compression overhead (~20x slower but realistic)

---

### 2. Transport Performance (`benchmark-transport-mitata.ts`)

**Purpose:** Measures end-to-end transport performance with middleware stack

**Metrics:**
- Mean latency (µs per operation)
- p75, p99, p999 percentiles
- Operations per second
- Memory allocation patterns

**What it tests:**
- Baseline (no middleware) vs. full middleware stack
- Inline headers vs. middleware headers
- Compression overhead
- Middleware execution time

**Run:** `npm run benchmark:transport`

### 2. GC Tracking (`benchmark-gc-tracking.ts`)

**Purpose:** Detailed garbage collection and heap usage analysis

**Metrics:**
- Total GC events (major/minor)
- GC overhead percentage
- Heap growth over time
- Memory allocation rate

**What it tests:**
- Memory efficiency
- GC pressure
- Heap stability
- Allocation patterns

**Run:** `npm run benchmark:gc`

### 3. Merge Strategies (`benchmark-merge-strategies.ts`)

**Purpose:** Compare 8 different object merging approaches for MiddlewareContext

**Strategies tested:**
1. Spread operator (current naive)
2. Object.assign
3. Conditional merge (current optimized)
4. Manual assignment
5. structuredClone
6. Immutable pattern
7. Prototype chain
8. Optimized fast path

**Scenarios:**
- Full context merge (headers + shared)
- Headers-only merge (most common)
- No-op merge (early return test)

**Run:** `npm run benchmark:merge`

### 4. Simple Transport Benchmark (`benchmark-transport-simple.js`)

**Purpose:** Pure JavaScript benchmark for profiling tools (0x, clinic)

**Why it exists:** Profiling tools work better with JavaScript than TypeScript. This is a simplified version specifically for generating flamegraphs.

**Run:** `npx 0x --output-dir=profile-results/flame -- node scripts/benchmark/benchmark-transport-simple.js`

---

## Profiling Tools

### Generate CPU Flamegraph (Recommended)

```bash
npm run benchmark:profile
# Select benchmark file when prompted
```

The flamegraph shows:
- **X-axis (width):** CPU time spent in each function
- **Y-axis (height):** Call stack depth
- **Interactive:** Click to zoom, hover for details

**Output:** `profile-results/flame/flamegraph.html`

### Available Profilers

```bash
npx tsx scripts/benchmark/profile-benchmark.ts <profiler> <benchmark-file>
```

**Profiler options:**
- `flame` - 0x flamegraph (interactive CPU profile)
- `prof` - Node.js built-in --prof
- `clinicDoctor` - Clinic.js overall health check
- `clinicFlame` - Clinic.js CPU flamegraph
- `clinicBubble` - Clinic.js async operations
- `inspect` - Chrome DevTools (interactive)

**Example:**
```bash
npx tsx scripts/benchmark/profile-benchmark.ts flame scripts/benchmark/benchmark-transport-mitata.ts
```

---

## Understanding Results

### Performance Targets

| Metric | Target | Current Status |
|--------|--------|----------------|
| Transport latency (mean) | < 50 µs | ✅ 24.37 µs (2.0x better) |
| Transport latency (p99) | < 100 µs | ✅ 27.31 µs (3.7x better) |
| GC overhead | < 2% | ✅ 1.50% (25% better) |
| Operations/sec | > 10,000 | ✅ 19,360 (1.9x better) |

### What the Flamegraph Shows

The flamegraph generated by profiling visualizes where CPU time is spent:

**Good signs:**
- Wide bars in Transport.request() - expected work
- Minimal time in logging/validation
- Flat profiles (not deeply nested)

**Warning signs:**
- Wide bars in unexpected places (logging, object creation)
- Very tall stacks (excessive function calls)
- Hot spots in utility functions

**How to investigate:**
1. Open `profile-results/flame/flamegraph.html` in a browser
2. Look for the widest bars (most CPU time)
3. Click on bars to zoom in
4. Verify wide bars represent intentional work (serialization, compression, network I/O setup)

### Key Performance Findings

**1. Middleware Stack Efficiency**
- Current implementation: 24.37 µs per operation
- Baseline (no middleware): 24.84 µs
- **Result:** Middleware is 2% faster with better p99 consistency

**2. Early Return Optimization**
- With early return: 131 ns
- Without early return: 348 ns  
- **Impact:** 2.66x speedup when no merge needed

**3. Header Processing**
- Inline headers: 4.41 ns
- Middleware headers: 941 ns
- **Ratio:** 213x difference (not critical for typical use)

**4. GC Efficiency**
- GC overhead: 1.50% (excellent)
- Major GC events: 1 per 10,000 operations
- Minor GC events: 50 per 10,000 operations

---

## File Structure

```
scripts/benchmark/
├── README.md                           # This file
├── BENCHMARK_RESULTS.md                # Detailed results and analysis
├── PROFILING_SUMMARY.md                # Executive summary for managers
├── PROFILING_RESULTS.md                # Profiling guide and interpretation
│
├── benchmark-transport-mitata.ts       # Full transport benchmark (TypeScript)
├── benchmark-gc-tracking.ts            # GC and memory tracking
├── benchmark-merge-strategies.ts       # Object merge comparison
├── benchmark-transport-simple.js       # Simple JS version for profiling
│
├── profile-benchmark.ts                # Profiling orchestration tool
├── save-baseline.ts                    # Save benchmark results as baseline
│
└── baselines/                          # Stored baseline results
    └── .gitkeep
```

---

## Baseline Management

### Save Current Results as Baseline

```bash
npm run benchmark:save-baseline
```

This captures current performance as a reference point for future comparisons.

### Compare Against Baseline

Baselines are automatically loaded and compared when running benchmarks. Results show:
- ✅ Pass (within threshold)
- ⚠️  Warning (5-10% regression)
- ❌ Fail (>10% regression)

---

## Best Practices

### When to Run Benchmarks

1. **Before major refactoring** - Establish baseline
2. **After optimization work** - Verify improvements
3. **Before releases** - Ensure no regressions
4. **When adding middleware** - Check overhead
5. **When changing merge logic** - Validate performance

### Interpreting Results

**Statistical significance:**
- mitata runs multiple iterations with warmup
- Reports mean, median, and percentiles
- Standard deviation shows consistency

**GC overhead:**
- Target: < 2%
- Warning: 2-5%
- Critical: > 5%

**Latency percentiles:**
- p50 (median): Typical case
- p99: 99% of operations faster than this
- p999: Worst 0.1% of cases

### CI/CD Integration

To add benchmarks to CI:

1. Run benchmarks on significant changes
2. Compare against saved baselines
3. Fail build if > 10% regression
4. Store results as artifacts

---

## Troubleshooting

### Benchmark is Too Fast/Slow

**Too fast** (< 1ms total):
- Increase iterations
- Add realistic work (compression, parsing)

**Too slow** (> 10s total):
- Reduce iterations
- Use smaller data payloads
- Profile to find bottlenecks

### Profiling Fails

**0x/clinic errors:**
- These tools work better with pure JavaScript
- Use `benchmark-transport-simple.js` instead
- Ensure Node.js version compatibility

**TypeScript profiling issues:**
- Compile to JavaScript first: `npm run build`
- Use built-in Node profiler: `node --prof`

### Inconsistent Results

**High variance:**
- Close other applications
- Run multiple times and average
- Check for thermal throttling
- Use `mitata` which handles warmup/JIT

---

## Advanced Usage

### Custom Benchmarks

Create a new benchmark file:

```typescript
import { bench, run, group } from 'mitata'

group('My Benchmark', () => {
  bench('test case 1', () => {
    // Your code here
  })
  
  bench('test case 2', async () => {
    // Async code
  })
})

run()
```

### Profile with Chrome DevTools

```bash
node --inspect-brk scripts/benchmark/benchmark-transport-simple.js
```

Then:
1. Open Chrome: `chrome://inspect`
2. Click "inspect" under Remote Target
3. Go to Profiler/Performance tab
4. Start recording
5. Continue execution

### Custom Profiling

```typescript
import { performance } from 'node:perf_hooks'

performance.mark('start')
// Your code
performance.mark('end')
performance.measure('duration', 'start', 'end')
```

---

## Contributing

When adding new benchmarks:

1. **Use mitata** for statistical analysis
2. **Include warmup** to allow JIT optimization
3. **Test realistic scenarios** (not just micro-benchmarks)
4. **Document what you're testing** and why
5. **Set performance targets** in documentation
6. **Add to npm scripts** for easy access

---

## Current Status

**Overall Performance:** ✅ Excellent

All performance targets exceeded. No critical optimization work required. Focus on:
- Maintaining current performance
- Monitoring for regressions
- Establishing baselines for future comparison

For detailed results, see:
- `BENCHMARK_RESULTS.md` - Full benchmark data
- `PROFILING_SUMMARY.md` - Executive summary
- `PROFILING_RESULTS.md` - Profiling interpretation guide
